{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/LLM_token_selection/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Adding idx column: 100%|██████████| 2000/2000 [00:00<00:00, 12171.62 examples/s]\n",
      "Tokenizing and reformatting instruction data: 100%|██████████| 2000/2000 [00:05<00:00, 379.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "dataset_name='random_1'\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "data_path = f\"selected_data/{dataset_name}.json\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "raw_dataset = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_prompt_completion_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= 2048,\n",
    "        add_bos= False,\n",
    "    )\n",
    "elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= 2048,\n",
    "        add_bos= False,\n",
    "    )\n",
    "    \n",
    "raw_dataset = raw_dataset.map(\n",
    "    lambda example, idx: {\"idx\": idx},\n",
    "    with_indices=True,  \n",
    "    desc=\"Adding idx column\",\n",
    ")\n",
    "        \n",
    "\n",
    "lm_datasets = raw_dataset.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_dataset['labels']\n",
    "\n",
    "all_len = 0\n",
    "\n",
    "for label in labels:\n",
    "    all_len += len(label)\n",
    "    \n",
    "all_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1625383/2437583171.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  losses_pre = torch.load(f\"token_losses_{dataset_name}_base.pt\")\n",
      "/tmp/ipykernel_1625383/2437583171.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  losses_new = torch.load(f\"token_losses_{dataset_name}_test.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "dataset_name='random'\n",
    "losses_pre = torch.load(f\"token_losses_{dataset_name}_base.pt\")\n",
    "losses_new = torch.load(f\"token_losses_{dataset_name}_test.pt\")\n",
    "\n",
    "start=1000\n",
    "length=1000\n",
    "loss_diff = []\n",
    "loss_HL_prop = []\n",
    "select_tokens_indices = []\n",
    "for loss1, loss2 in zip(losses_pre[start:start+length], losses_new[start:start+length]):\n",
    "    # print(f\"shape1: {len(loss1)}; shape2: {len(loss2)}\")\n",
    "    diff = np.array(loss1)-np.array(loss2)\n",
    "    loss_diff.append(diff)\n",
    "    _, indices = torch.topk(torch.tensor(diff), k=len(diff)//2)\n",
    "    select_tokens_indices.append((indices + 1).tolist()) ## indices +1 represents the biased value, which match the real token in the original dataset\n",
    "    loss_HL_prop.append(round(np.sum(diff>0)/len(diff) * 100, 3))\n",
    "    \n",
    "# dataset = load_dataset(\"json\", data_files=\"selected_data/meta-llama/Meta-Llama-3.1-8B-Instruct/all_train/random_dataset.json\")\n",
    "train_dataset['labels']\n",
    "\n",
    "new_labels=[]\n",
    "for selected_indices, label in zip(select_tokens_indices, train_dataset['labels'][start:start+length]):\n",
    "    # print(f\"selected indices: {len(selected_indices)};; label: {len(label)}\")\n",
    "    new_label = [-100] * len(label)\n",
    "    for idx in sorted(selected_indices):\n",
    "        new_label[idx] = label[idx]\n",
    "    new_labels.append(new_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['labels'][start:start+length] = new_labels ##need to determine how to convert to labels to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into several subsets for multiple epoch running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10000 examples [00:00, 86674.93 examples/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 25.49ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 26.00ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 35.97ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 25.15ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 34.96ba/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "dataset_name = 'filtered-cured'\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "data_size = 2000\n",
    "subset_size = len(dataset) // data_size\n",
    "\n",
    "\n",
    "for i in range(subset_size):\n",
    "    selected_indices = [idx for idx in range(data_size *i, data_size * (i+1))]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}_{i}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
