{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding idx column: 100%|██████████| 1000/1000 [00:00<00:00, 17192.73 examples/s]\n",
      "Tokenizing and reformatting instruction data: 100%|██████████| 1000/1000 [00:02<00:00, 478.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "dataset_name='test_dataset'\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "data_path = f\"selected_data/{dataset_name}.json\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "raw_dataset = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_prompt_completion_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= 2048,\n",
    "        add_bos= False,\n",
    "    )\n",
    "elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= 2048,\n",
    "        add_bos= False,\n",
    "    )\n",
    "    \n",
    "raw_dataset = raw_dataset.map(\n",
    "    lambda example, idx: {\"idx\": idx},\n",
    "    with_indices=True,  \n",
    "    desc=\"Adding idx column\",\n",
    ")\n",
    "        \n",
    "\n",
    "lm_datasets = raw_dataset.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'id', 'messages', 'idx', 'input_ids', 'labels', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into several subsets for multiple epoch running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21f475e7f364ee2b41d8430933a0fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa0303bcf9844b88f9f4f5ae223e4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d934d58dd96844b380c44cac54aa1913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d212e9a40f1949809252aa2ad68249a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f533f61759e40aa912808c276a61d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "dataset_name = 'filtered-cured-50k'\n",
    "# dataset_name = \"random_subset_50k\"\n",
    "# dataset_name = \"alpaca_52k\"\n",
    "# dataset_name = \"full\"\n",
    "# dataset_name = \"filtered-cured-10k\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "# dataset = dataset.shuffle(seed=42) \n",
    "# dataset = dataset.select(range(len(dataset)-1, -1, -1))\n",
    "\n",
    "\n",
    "subset_size = 5\n",
    "\n",
    "data_size = len(dataset) // subset_size\n",
    "# data_size = 10\n",
    "for i in range(subset_size):\n",
    "    selected_indices = [idx for idx in range(data_size *i, data_size * (i+1))]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-smooth-positive-reverse_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-global-fixed-base-loss_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-positive-new1-fixed-base-loss_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-sample-positive-reverse_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-positive-new2-shuffle_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-token_ranking_sample_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-token_ranking_sample-fixed-base-loss_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-half-positive-fixed-base-loss_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-positive-fixed-base-loss-using-warmup_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-positive-using-warmup_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-positive-fixed-base-loss-using-warmup-full-data_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-positive-using-full-train-reference_{i}.json\")\n",
    "    \n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-half-positive-using-full-train-reference_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-curve-positive-new1-lr_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_combine_global_half_positive_fixed_based_loss_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_combine_active_split_global_half_positive_{i}.json\")\n",
    "    \n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_combine_active-split-global-curve-positive-fixed-base-loss-using-warmup-label_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_combine_active-split-global-curve-positive-fixed-base-loss-label_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_0.3_{i}.json\")\n",
    "    \n",
    "    # subset.to_json(data_path + f\"{dataset_name}-active-split-global-half-positive-fixed-base-loss-using-warmup_{i}.json\")\n",
    "    \n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-label_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_0.3_combine_other_two_types_label_{i}.json\")\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-iter-split-global_data_prop_0.6_combine_other_two_types_label_{i}.json\")\n",
    "    subset.to_json(data_path + f\"{dataset_name}-iter-split-sample_data_prop_0.6_{i}.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4faeed70954c6280339887f7966fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9004f4c1544c3c8661b9fb859df9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a800aa0ce574c3d96a15be3a46b57bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d59f0391bc42a0bc0258289ea01aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f494b616f0ac49e0a75a0e847ac38f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "dataset_name = 'filtered-cured-50k'\n",
    "exp_tag = \"non-iter-split-global-new-randtok\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "first_subset_size = 5000  # 第一个子集的大小\n",
    "\n",
    "# 计算剩余数据的大小\n",
    "remaining_data_size = len(dataset) - first_subset_size\n",
    "num_iter = 4\n",
    "subset_size = remaining_data_size // num_iter  # 将剩余数据平分为剩余的子集数量\n",
    "print(subset_size)\n",
    "# 第一个子集\n",
    "first_subset = dataset.select(range(first_subset_size))\n",
    "first_subset.to_json(data_path + f\"{dataset_name}-{exp_tag}_0.json\")\n",
    "\n",
    "# 后续的子集\n",
    "for i in range(num_iter):\n",
    "    start_idx = first_subset_size + i * subset_size\n",
    "    end_idx = start_idx + subset_size\n",
    "    selected_indices = list(range(start_idx, end_idx))\n",
    "    subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-{exp_tag}_{i+1}.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37a016c5ef04367b0eee6955a0dd4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab011596f843698c472917556b1a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58135220f2754945bd4de75973ac6e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a0ed27aefa4bdabe32e305d21d2900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233a736c941b48b1b31954f3417c6c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TemporarilySeededRandom:\n",
    "    def __init__(self, seed):\n",
    "        \"\"\"Temporarily set the random seed, and then restore it when exiting the context.\"\"\"\n",
    "        self.seed = seed\n",
    "        self.stored_state = None\n",
    "        self.stored_np_state = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Store the current random state\n",
    "        self.stored_state = random.getstate()\n",
    "        self.stored_np_state = np.random.get_state()\n",
    "\n",
    "        # Set the random seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the random state\n",
    "        random.setstate(self.stored_state)\n",
    "        np.random.set_state(self.stored_np_state)\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'random' #'filtered-cured'\n",
    "dataset_name = 'filtered-cured-50k'#'filtered-cured'\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "\n",
    "### num_iters\n",
    "num_iters=5\n",
    "\n",
    "subset_size = int(len(train_dataset) * 0.01)\n",
    "\n",
    "for idx in range(num_iters):\n",
    "    \n",
    "    if idx > 0:        \n",
    "        # with TemporarilySeededRandom(idx * 10086):\n",
    "        #     random_indices = np.random.choice(len(train_dataset), size=subset_size*6, replace=False)\n",
    "        # subset = train_dataset.select(random_indices)\n",
    "        \n",
    "        subset = train_dataset        \n",
    "\n",
    "    else: ## for all token selection with subset\n",
    "        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*2, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "\n",
    "\n",
    "    # subset = dataset.select(selected_indices)\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-all-non-iter-sample-subset-new_{idx}.json\")\n",
    "    subset.to_json(data_path + f\"{dataset_name}-all-non-iter-global_{idx}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global level top-k data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3cc8038e834f4f9c06a18538095aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81591372bf5b4822949ca7dda9660af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e192a2a30a4594b400059b49447b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f079079a759466bb3fbe15931365695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b020039bd340a7b3b6dcd23d7927d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec94edbb412e4629a8cfb4cc9dead391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cdf672a9ac4624ba931ba9bc75b0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f354a3d0b9934f9fa7b29ada654ac3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769bad81c0524630a38fd4a990e161bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcae194be10b402b8735ef9dbf0867b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd17cca9e5c44dc5878b718ad4fe4f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TemporarilySeededRandom:\n",
    "    def __init__(self, seed):\n",
    "        \"\"\"Temporarily set the random seed, and then restore it when exiting the context.\"\"\"\n",
    "        self.seed = seed\n",
    "        self.stored_state = None\n",
    "        self.stored_np_state = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Store the current random state\n",
    "        self.stored_state = random.getstate()\n",
    "        self.stored_np_state = np.random.get_state()\n",
    "\n",
    "        # Set the random seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the random state\n",
    "        random.setstate(self.stored_state)\n",
    "        np.random.set_state(self.stored_np_state)\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'random' #'filtered-cured'\n",
    "dataset_name = 'filtered-cured-50k'#'filtered-cured'\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "data_type_tag='combine_loss' ##global sample union additional_two_tokens intersection\n",
    "### num_iters\n",
    "num_iters=10\n",
    "\n",
    "subset_size = int(len(train_dataset) * 0.01)\n",
    "\n",
    "for idx in range(num_iters):\n",
    "    \n",
    "    if idx % 2 == 1:        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*6, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "        \n",
    "    else: ## for all token selection with subset\n",
    "        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*2, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "\n",
    "\n",
    "    # subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-all-iter-{data_type_tag}-subset-small-new_{idx}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-iteration form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578eba6a13f54a69a2eccf8b00296011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e079c52f38a476dbe969932b3256b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57256f3e3d544772a8d2d56196428e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a312292a5a1847f6b55a9579190bdc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d298af055c40dea04dd64a3dde6f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TemporarilySeededRandom:\n",
    "    def __init__(self, seed):\n",
    "        \"\"\"Temporarily set the random seed, and then restore it when exiting the context.\"\"\"\n",
    "        self.seed = seed\n",
    "        self.stored_state = None\n",
    "        self.stored_np_state = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Store the current random state\n",
    "        self.stored_state = random.getstate()\n",
    "        self.stored_np_state = np.random.get_state()\n",
    "\n",
    "        # Set the random seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the random state\n",
    "        random.setstate(self.stored_state)\n",
    "        np.random.set_state(self.stored_np_state)\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'random' #'filtered-cured'\n",
    "dataset_name = 'filtered-cured-50k'#'filtered-cured'\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "\n",
    "### num_iters\n",
    "num_iters=5\n",
    "\n",
    "subset_size = int(len(train_dataset) * 0.01)\n",
    "\n",
    "for idx in range(num_iters):\n",
    "    \n",
    "    if idx > 0:        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*6, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "        \n",
    "    else: ## for all token selection with subset\n",
    "        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*2, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "\n",
    "\n",
    "    # subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-all-non-iter-sample-subset-new_{idx}.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the text of selected-token (text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67aa247ba084d659774765c8709ffd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a6b8a17d694a0db8633e5dcd3c3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/label/token_labels_filtered-cured-50k-active-split-global-curve-positive-new1_1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jlpang/LLM_token_selection/data_preprocess.ipynb 单元格 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/LLM_token_selection/data_preprocess.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m raw_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m, data_files\u001b[39m=\u001b[39m data_path \u001b[39m+\u001b[39m cur_orig_data)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/LLM_token_selection/data_preprocess.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m orig_labels_all \u001b[39m=\u001b[39m encode_data(raw_dataset)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/LLM_token_selection/data_preprocess.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m cur_labels_all \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(label_path \u001b[39m+\u001b[39;49m cur_model_label, weights_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/LLM_token_selection/data_preprocess.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m text_all \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/LLM_token_selection/data_preprocess.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n",
      "File \u001b[0;32m~/LLM_token_selection/venv/lib/python3.10/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/LLM_token_selection/venv/lib/python3.10/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    660\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/LLM_token_selection/venv/lib/python3.10/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/label/token_labels_filtered-cured-50k-active-split-global-curve-positive-new1_1.pt'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm \n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_data(raw_dataset):\n",
    "    \n",
    "    if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_prompt_completion_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "    elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_messages_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "        \n",
    "            \n",
    "    lm_datasets = raw_dataset.map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "\n",
    "    return lm_datasets['train']['labels']\n",
    "    \n",
    "###############################################\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "\n",
    "train_dataset_tag = \"filtered-cured-50k-active-split-global-curve-positive-new1\"\n",
    "\n",
    "# train_dataset_tag = \"filtered-cured-50k-active-split-global-curve-positive-new1-fixed-base-loss\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_path = 'results/label/'\n",
    "data_path = f\"selected_data/\"\n",
    "\n",
    "text_all_files = {}\n",
    "for idx in range(1,2):\n",
    "    cur_model_label = f'token_labels_{train_dataset_tag}' + \"_\" + str(idx) + '.pt'\n",
    "    cur_orig_data = train_dataset_tag + \"_\" + str(idx) + '.json'\n",
    "    \n",
    "    raw_dataset = load_dataset(\"json\", data_files= data_path + cur_orig_data)\n",
    "    \n",
    "    orig_labels_all = encode_data(raw_dataset)\n",
    "\n",
    "    cur_labels_all = torch.load(label_path + cur_model_label, weights_only=False)\n",
    "    \n",
    "    text_all = []\n",
    "    count = 20\n",
    "\n",
    "    for cur_labels, orig_labels in tqdm(zip(cur_labels_all, orig_labels_all), desc=\"handing each sample\"):\n",
    "        text = []\n",
    "        for cur_label, orig_label in zip(cur_labels, orig_labels):\n",
    "            \n",
    "            token = tokenizer.decode([orig_label], skip_special_tokens=True)  \n",
    "            if cur_label != -100:  # highlight \n",
    "                text.append(colored(token, 'red'))\n",
    "            else:\n",
    "                text.append(token)\n",
    "                \n",
    "                \n",
    "        text_single = \"\".join(text)\n",
    "        \n",
    "        if count > 0:\n",
    "            print('#' * 200 + '\\n')\n",
    "            print(text_single)\n",
    "        count -= 1\n",
    "        \n",
    "        text_all.append(text_single)\n",
    "        \n",
    "    text_all_files[idx] = text_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the all tokens selected by global or sample-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1-th file:: ### intersection_labels ### label proportion::  26.16 %\n",
      "dataset 1-th file:: ### union_labels ### label proportion::  33.69 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a99862d31554c999261529da3b57fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 2-th file:: ### intersection_labels ### label proportion::  26.09 %\n",
      "dataset 2-th file:: ### union_labels ### label proportion::  33.76 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aa32e434d34a43911aefc47e5976ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 3-th file:: ### intersection_labels ### label proportion::  26.4 %\n",
      "dataset 3-th file:: ### union_labels ### label proportion::  33.44 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d0cbf5bbc94f14b0960a4f3abf3445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 4-th file:: ### intersection_labels ### label proportion::  26.36 %\n",
      "dataset 4-th file:: ### union_labels ### label proportion::  33.48 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25220c5f6def493ba0eecb24b4499eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 5-th file:: ### intersection_labels ### label proportion::  26.53 %\n",
      "dataset 5-th file:: ### union_labels ### label proportion::  33.32 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb537da9467546f2a66acfe7dbdcf41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 6-th file:: ### intersection_labels ### label proportion::  26.35 %\n",
      "dataset 6-th file:: ### union_labels ### label proportion::  33.5 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4690cee172b6406d8ca1ec98acbba5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 7-th file:: ### intersection_labels ### label proportion::  26.44 %\n",
      "dataset 7-th file:: ### union_labels ### label proportion::  33.41 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3db37900322491991019d1b839c4688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 8-th file:: ### intersection_labels ### label proportion::  26.44 %\n",
      "dataset 8-th file:: ### union_labels ### label proportion::  33.41 %\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm \n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_data(raw_dataset):\n",
    "    \n",
    "    if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_prompt_completion_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "    elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_messages_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "        \n",
    "            \n",
    "    lm_datasets = raw_dataset.map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "\n",
    "    return lm_datasets['train']['labels']\n",
    "    \n",
    "    \n",
    "    \n",
    "def counting_labels(labels_all, special_token=-100):\n",
    "    all_counts = 0\n",
    "    selected_count = 0\n",
    "    for labels in labels_all:\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                selected_count +=1\n",
    "                \n",
    "        all_counts += len(labels)\n",
    "    return selected_count, all_counts\n",
    "\n",
    "###############################################\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "label_path = 'results/label/'\n",
    "data_path = f\"selected_data/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "text_all_files = {}\n",
    "\n",
    "for idx in range(1,9):\n",
    "    \n",
    "    #original labels\n",
    "    data_tag=f\"filtered-cured-50k-all-iter-global-subset-small-new_\"\n",
    "    cur_orig_data = data_tag + str(idx) + '.json'\n",
    "    raw_dataset = load_dataset(\"json\", data_files= data_path + cur_orig_data)\n",
    "    orig_labels_all = encode_data(raw_dataset)\n",
    "\n",
    "\n",
    "    labels_all_levels = {}\n",
    "    \n",
    "    for train_tag in ['global', 'sample']:\n",
    "        \n",
    "        # print(f\"#### Current Token Selection Pattern: {train_tag}-level #####\")\n",
    "        label_tag=f'token_labels_filtered-cured-50k-all-iter-{train_tag}-subset-small-new_'\n",
    "        cur_model_label = label_tag + str(idx) + '.pt'\n",
    "        cur_labels_all = torch.load(label_path + cur_model_label, weights_only=False)\n",
    "\n",
    "        labels_all_levels[train_tag] = cur_labels_all\n",
    "\n",
    "    intersection_labels_all = [] ## the same selected labels\n",
    "    union_labels_all = [] ## the labels selected by global or sample\n",
    "    \n",
    "    for global_labels, sample_labels in zip(labels_all_levels['global'], labels_all_levels['sample']):\n",
    "        intersection_labels = [-100] * len(global_labels)\n",
    "        union_labels = [-100] * len(global_labels)\n",
    "\n",
    "        for i, (global_label, sample_label) in enumerate(zip(global_labels, sample_labels)):\n",
    "            if global_label != -100 or sample_label != -100:\n",
    "                selected_label = global_label if global_label != -100 else sample_label\n",
    "                union_labels[i] = selected_label\n",
    "                \n",
    "            if global_label != -100 and sample_label != -100: \n",
    "                if global_label == sample_label:               \n",
    "                    intersection_labels[i] = sample_label\n",
    "        \n",
    "        intersection_labels_all.append(intersection_labels)\n",
    "        union_labels_all.append(union_labels)\n",
    "        \n",
    "    text_all_files[idx] = {\n",
    "        'intersection_labels': intersection_labels_all,\n",
    "        'union_labels': union_labels_all\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ### compute the proportion of labels\n",
    "    for key, item in text_all_files[idx].items():\n",
    "        \n",
    "        selected_count, all_counts = counting_labels(item)\n",
    "        print(f\"dataset {idx}-th file:: ### {key} ### label proportion::  {round(selected_count/all_counts * 100, 2)} %\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinlong/LLM_token_selection/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [06:21<00:00, 95.49s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name_or_path= \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3.1-8B-Instruct'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_name=\"meta-llama/Llama-3.1-8B-Instruct.json\"\n",
    "path = os.path.basename(path_name).split(\".json\")[0]\n",
    "path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3571799/550240146.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(\"results/label/token_labels_filtered-cured-50k-active-split_0.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.load(\"results/label/token_labels_filtered-cured-50k-active-split_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allenai/tulu-v2-sft-mixture\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 326154/326154 [00:07<00:00, 42758.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 20016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 过滤出 dataset 列中值为 'alpaca_eval' 的数据\n",
    "alpaca_eval_data = dataset.filter(lambda x: x['dataset'] == 'code_alpaca')\n",
    "\n",
    "# 输出过滤后的数据集大小\n",
    "print(\"Filtered dataset size:\", len(alpaca_eval_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
