{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "dataset_name='test_dataset'\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "data_path = f\"selected_data/{dataset_name}.json\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "raw_dataset = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_prompt_completion_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= 2048,\n",
    "        add_bos= False,\n",
    "    )\n",
    "elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= 2048,\n",
    "        add_bos= False,\n",
    "    )\n",
    "    \n",
    "raw_dataset = raw_dataset.map(\n",
    "    lambda example, idx: {\"idx\": idx},\n",
    "    with_indices=True,  \n",
    "    desc=\"Adding idx column\",\n",
    ")\n",
    "        \n",
    "\n",
    "lm_datasets = raw_dataset.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into several subsets for multiple epoch running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-2300958McomFZ0daFd.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'filtered-cured-50k'\n",
    "# dataset_name = \"random_subset_50k\"\n",
    "# dataset_name = \"alpaca_52k\"\n",
    "# dataset_name = \"full\"\n",
    "# dataset_name = \"filtered-cured-10k\"\n",
    "dataset_name = \"test_100\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "token_select_pattern= \"test\" #\"token_ranking_sample\" #\"global-curve-positive-new1\"\n",
    "\n",
    "subset_size = 5\n",
    "\n",
    "data_size = len(dataset) // subset_size\n",
    "# data_size = 10\n",
    "\n",
    "for i in range(subset_size):\n",
    "    selected_indices = [idx for idx in range(data_size *i, data_size * (i+1))]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-active-split-{token_select_pattern}_{i}.json\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "dataset_name = 'filtered-cured-50k'\n",
    "exp_tag = \"non-iter-split-global-new-randtok\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "first_subset_size = 5000  # 第一个子集的大小\n",
    "\n",
    "# 计算剩余数据的大小\n",
    "remaining_data_size = len(dataset) - first_subset_size\n",
    "num_iter = 4\n",
    "subset_size = remaining_data_size // num_iter  # 将剩余数据平分为剩余的子集数量\n",
    "print(subset_size)\n",
    "# 第一个子集\n",
    "first_subset = dataset.select(range(first_subset_size))\n",
    "first_subset.to_json(data_path + f\"{dataset_name}-{exp_tag}_0.json\")\n",
    "\n",
    "# 后续的子集\n",
    "for i in range(num_iter):\n",
    "    start_idx = first_subset_size + i * subset_size\n",
    "    end_idx = start_idx + subset_size\n",
    "    selected_indices = list(range(start_idx, end_idx))\n",
    "    subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-{exp_tag}_{i+1}.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TemporarilySeededRandom:\n",
    "    def __init__(self, seed):\n",
    "        \"\"\"Temporarily set the random seed, and then restore it when exiting the context.\"\"\"\n",
    "        self.seed = seed\n",
    "        self.stored_state = None\n",
    "        self.stored_np_state = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Store the current random state\n",
    "        self.stored_state = random.getstate()\n",
    "        self.stored_np_state = np.random.get_state()\n",
    "\n",
    "        # Set the random seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the random state\n",
    "        random.setstate(self.stored_state)\n",
    "        np.random.set_state(self.stored_np_state)\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'random' #'filtered-cured'\n",
    "dataset_name = 'filtered-cured-50k'#'filtered-cured'\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "\n",
    "### num_iters\n",
    "num_iters=5\n",
    "\n",
    "subset_size = int(len(train_dataset) * 0.01)\n",
    "\n",
    "for idx in range(num_iters):\n",
    "    \n",
    "    if idx > 0:        \n",
    "        # with TemporarilySeededRandom(idx * 10086):\n",
    "        #     random_indices = np.random.choice(len(train_dataset), size=subset_size*6, replace=False)\n",
    "        # subset = train_dataset.select(random_indices)\n",
    "        \n",
    "        subset = train_dataset        \n",
    "\n",
    "    else: ## for all token selection with subset\n",
    "        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*2, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "\n",
    "\n",
    "    # subset = dataset.select(selected_indices)\n",
    "    # subset.to_json(data_path + f\"{dataset_name}-all-non-iter-sample-subset-new_{idx}.json\")\n",
    "    subset.to_json(data_path + f\"{dataset_name}-all-non-iter-global_{idx}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global level top-k data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TemporarilySeededRandom:\n",
    "    def __init__(self, seed):\n",
    "        \"\"\"Temporarily set the random seed, and then restore it when exiting the context.\"\"\"\n",
    "        self.seed = seed\n",
    "        self.stored_state = None\n",
    "        self.stored_np_state = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Store the current random state\n",
    "        self.stored_state = random.getstate()\n",
    "        self.stored_np_state = np.random.get_state()\n",
    "\n",
    "        # Set the random seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the random state\n",
    "        random.setstate(self.stored_state)\n",
    "        np.random.set_state(self.stored_np_state)\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'random' #'filtered-cured'\n",
    "dataset_name = 'filtered-cured-50k'#'filtered-cured'\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "data_type_tag='combine_loss' ##global sample union additional_two_tokens intersection\n",
    "### num_iters\n",
    "num_iters=10\n",
    "\n",
    "subset_size = int(len(train_dataset) * 0.01)\n",
    "\n",
    "for idx in range(num_iters):\n",
    "    \n",
    "    if idx % 2 == 1:        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*6, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "        \n",
    "    else: ## for all token selection with subset\n",
    "        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*2, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "\n",
    "\n",
    "    # subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-all-iter-{data_type_tag}-subset-small-new_{idx}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-iteration form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TemporarilySeededRandom:\n",
    "    def __init__(self, seed):\n",
    "        \"\"\"Temporarily set the random seed, and then restore it when exiting the context.\"\"\"\n",
    "        self.seed = seed\n",
    "        self.stored_state = None\n",
    "        self.stored_np_state = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Store the current random state\n",
    "        self.stored_state = random.getstate()\n",
    "        self.stored_np_state = np.random.get_state()\n",
    "\n",
    "        # Set the random seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the random state\n",
    "        random.setstate(self.stored_state)\n",
    "        np.random.set_state(self.stored_np_state)\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'selected_data/'\n",
    "\n",
    "# dataset_name = 'random' #'filtered-cured'\n",
    "dataset_name = 'filtered-cured-50k'#'filtered-cured'\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files= data_path + f'{dataset_name}_dataset.json')['train']\n",
    "\n",
    "\n",
    "### num_iters\n",
    "num_iters=5\n",
    "\n",
    "subset_size = int(len(train_dataset) * 0.01)\n",
    "\n",
    "for idx in range(num_iters):\n",
    "    \n",
    "    if idx > 0:        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*6, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "        \n",
    "    else: ## for all token selection with subset\n",
    "        \n",
    "        with TemporarilySeededRandom(idx * 10086):\n",
    "            random_indices = np.random.choice(len(train_dataset), size=subset_size*2, replace=False)\n",
    "            \n",
    "        subset = train_dataset.select(random_indices)\n",
    "\n",
    "\n",
    "    # subset = dataset.select(selected_indices)\n",
    "    subset.to_json(data_path + f\"{dataset_name}-all-non-iter-sample-subset-new_{idx}.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the text of selected-token (text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm \n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_data(raw_dataset):\n",
    "    \n",
    "    if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_prompt_completion_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "    elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_messages_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "        \n",
    "            \n",
    "    lm_datasets = raw_dataset.map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "\n",
    "    return lm_datasets['train']['labels']\n",
    "    \n",
    "###############################################\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "train_tag='sample'\n",
    "\n",
    "print(f\"#### Current Token Selection Pattern: {train_tag}-level #####\")\n",
    "\n",
    "label_tag=f'token_labels_filtered-cured-50k-all-iter-{train_tag}-subset-small-new_'\n",
    "data_tag=f\"filtered-cured-50k-all-iter-{train_tag}-subset-small-new_\"\n",
    "\n",
    "label_path = 'results/label/'\n",
    "data_path = f\"selected_data/\"\n",
    "\n",
    "text_all_files = {}\n",
    "for idx in range(1,2):\n",
    "    cur_model_label = label_tag + str(idx) + '.pt'\n",
    "    cur_orig_data = data_tag + str(idx) + '.json'\n",
    "    \n",
    "    raw_dataset = load_dataset(\"json\", data_files= data_path + cur_orig_data)\n",
    "    \n",
    "    orig_labels_all = encode_data(raw_dataset)\n",
    "\n",
    "    cur_labels_all = torch.load(label_path + cur_model_label, weights_only=False)\n",
    "    \n",
    "    text_all = []\n",
    "    count = 20\n",
    "\n",
    "    for cur_labels, orig_labels in tqdm(zip(cur_labels_all, orig_labels_all), desc=\"handing each sample\"):\n",
    "        text = []\n",
    "        for cur_label, orig_label in zip(cur_labels, orig_labels):\n",
    "            \n",
    "            token = tokenizer.decode([orig_label], skip_special_tokens=True)  \n",
    "            if cur_label != -100:  # highlight \n",
    "                text.append(colored(token, 'red'))\n",
    "            else:\n",
    "                text.append(token)\n",
    "                \n",
    "                \n",
    "        text_single = \"\".join(text)\n",
    "        \n",
    "        if count > 0:\n",
    "            print('#' * 200 + '\\n')\n",
    "            print(text_single)\n",
    "        count -= 1\n",
    "        \n",
    "        text_all.append(text_single)\n",
    "        \n",
    "    text_all_files[idx] = text_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the all tokens selected by global or sample-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm \n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    # labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            # labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_data(raw_dataset):\n",
    "    \n",
    "    if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_prompt_completion_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "    elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_messages_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "        \n",
    "            \n",
    "    lm_datasets = raw_dataset.map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "\n",
    "    return lm_datasets['train']['labels']\n",
    "    \n",
    "    \n",
    "    \n",
    "def counting_labels(labels_all, special_token=-100):\n",
    "    all_counts = 0\n",
    "    selected_count = 0\n",
    "    for labels in labels_all:\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                selected_count +=1\n",
    "                \n",
    "        all_counts += len(labels)\n",
    "    return selected_count, all_counts\n",
    "\n",
    "###############################################\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "label_path = 'results/label/'\n",
    "data_path = f\"selected_data/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "text_all_files = {}\n",
    "\n",
    "for idx in range(1,9):\n",
    "    \n",
    "    #original labels\n",
    "    data_tag=f\"filtered-cured-50k-all-iter-global-subset-small-new_\"\n",
    "    cur_orig_data = data_tag + str(idx) + '.json'\n",
    "    raw_dataset = load_dataset(\"json\", data_files= data_path + cur_orig_data)\n",
    "    orig_labels_all = encode_data(raw_dataset)\n",
    "\n",
    "\n",
    "    labels_all_levels = {}\n",
    "    \n",
    "    for train_tag in ['global', 'sample']:\n",
    "        \n",
    "        # print(f\"#### Current Token Selection Pattern: {train_tag}-level #####\")\n",
    "        label_tag=f'token_labels_filtered-cured-50k-all-iter-{train_tag}-subset-small-new_'\n",
    "        cur_model_label = label_tag + str(idx) + '.pt'\n",
    "        cur_labels_all = torch.load(label_path + cur_model_label, weights_only=False)\n",
    "\n",
    "        labels_all_levels[train_tag] = cur_labels_all\n",
    "\n",
    "    intersection_labels_all = [] ## the same selected labels\n",
    "    union_labels_all = [] ## the labels selected by global or sample\n",
    "    \n",
    "    for global_labels, sample_labels in zip(labels_all_levels['global'], labels_all_levels['sample']):\n",
    "        intersection_labels = [-100] * len(global_labels)\n",
    "        union_labels = [-100] * len(global_labels)\n",
    "\n",
    "        for i, (global_label, sample_label) in enumerate(zip(global_labels, sample_labels)):\n",
    "            if global_label != -100 or sample_label != -100:\n",
    "                selected_label = global_label if global_label != -100 else sample_label\n",
    "                union_labels[i] = selected_label\n",
    "                \n",
    "            if global_label != -100 and sample_label != -100: \n",
    "                if global_label == sample_label:               \n",
    "                    intersection_labels[i] = sample_label\n",
    "        \n",
    "        intersection_labels_all.append(intersection_labels)\n",
    "        union_labels_all.append(union_labels)\n",
    "        \n",
    "    text_all_files[idx] = {\n",
    "        'intersection_labels': intersection_labels_all,\n",
    "        'union_labels': union_labels_all\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ### compute the proportion of labels\n",
    "    for key, item in text_all_files[idx].items():\n",
    "        \n",
    "        selected_count, all_counts = counting_labels(item)\n",
    "        print(f\"dataset {idx}-th file:: ### {key} ### label proportion::  {round(selected_count/all_counts * 100, 2)} %\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name_or_path= \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_name=\"meta-llama/Llama-3.1-8B-Instruct.json\"\n",
    "path = os.path.basename(path_name).split(\".json\")[0]\n",
    "path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.load(\"results/label/token_labels_filtered-cured-50k-active-split_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allenai/tulu-v2-sft-mixture\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "set(dataset['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.9.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOENT: no such file or directory, open '/tmp/kernel-v2-229374708knJWiFxGI.json'"
     ]
    }
   ],
   "source": [
    "# 过滤出 dataset 列中值为 'alpaca_eval' 的数据\n",
    "alpaca_eval_data = dataset.filter(lambda x: x['dataset'] == 'code_alpaca')\n",
    "\n",
    "# 输出过滤后的数据集大小\n",
    "print(\"Filtered dataset size:\", len(alpaca_eval_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
