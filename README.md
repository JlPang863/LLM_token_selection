## Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning
- Reference: [Not All Tokens Are What You Need for Pretraining](https://openreview.net/pdf?id=0NMzBwqaAJ), NeurIPS 2024 best paper runner up.



<!-- <br>
<p align="center">
<img src="pipeline_overview.png" width="00">
</p>
<br> -->

![The Overview of Token Cleaning Pipelines](token_selection_2.0.pdf)


## Data Preparation
<!-- 
One can check the `data_preprocess.ipynb` -->


## Dataset preparation

The data pool (50k samples) is constructed based on a new powerful data curation pipeline proposed by [DS$^2$](https://openreview.net/pdf?id=DKkQtRMowq), which involves selecting data samples using quality rating scores generated by LLMs. For convenience, the 50k used samples can be accessed from Huggingface via the [link](https://huggingface.co/datasets/jlpang888/DS2_50k).

Our selected evaluation and training data are listed below.

| **Category**         | **Dataset**                                  |
|----------------------|----------------------------------------------|
| **Evaluation Data**   | MMLU, TruthfulQA, TydiQA, HellaSwag, BoolQ, ARC-C, LoqiQA|
| **Training Data**     | Flan v2, OASST1, WizardLM, Dolly, Stanford Alpaca |


## Code Running

Note that our cleaning pipelines consists of **Fixed-Model Cleaning** and **Self-Evolving Cleaning**. One can run the code by 

```bash
# Fixed-model cleaning
bash run_rho_baseline_global.sh

# Self-evolving cleaning
bash run_iter_pattern_new.sh

