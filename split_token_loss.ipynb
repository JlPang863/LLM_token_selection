{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the orginal token loss to multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2042683/407093233.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reference_losses = torch.load(data_path + f\"token_losses_filtered-cured-50k-rho-baseline_{ref_name}.pt\")\n",
      "/tmp/ipykernel_2042683/407093233.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  base_losses = torch.load(data_path + f\"token_losses_filtered-cured-50k-rho-baseline_{base_name}.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "data_path = \"results/loss/\"\n",
    "\n",
    "### llama-3b\n",
    "# base_model_name_or_path=\"meta-llama/Llama-3.2-3B\"\n",
    "# reference_model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# reference_model_name_or_path=\"/mnt/data1/jinlong/token_selection_output/models/meta-llama/Llama-3.2-3B/data_prop_0.6/lora_merged_filtered-cured-10k-shuffle-warmup\"\n",
    "# reference_model_name_or_path=\"/mnt/data1/jinlong/token_selection_output/models/meta-llama/Llama-3.2-3B/data_prop_0.6/lora_merged_filtered-cured-50k-full-baseline\"\n",
    "\n",
    "#### llama-3b\n",
    "base_model_name_or_path=\"meta-llama/Llama-3.2-3B\"\n",
    "reference_model_name_or_path=f\"/mnt/data1/jinlong/token_selection_output/models/{base_model_name_or_path}/data_prop_0.6/lora_merged_filtered-cured-10k-warmup-llama3b\"\n",
    "\n",
    "\n",
    "#### llama-8b\n",
    "# base_model_name_or_path=\"meta-llama/Llama-3.1-8B\"\n",
    "# reference_model_name_or_path=f\"/mnt/data1/jinlong/token_selection_output/models/{base_model_name_or_path}/data_prop_0.6/lora_merged_filtered-cured-10k-warmup\"\n",
    "\n",
    "#### mistral\n",
    "# base_model_name_or_path=\"mistralai/Mistral-7B-v0.3\"\n",
    "# reference_model_name_or_path=f\"/mnt/data1/jinlong/token_selection_output/models/{base_model_name_or_path}/data_prop_0.6/lora_merged_filtered-cured-10k-warmup\"\n",
    "\n",
    "\n",
    "### dataset name that needed\n",
    "# train_dataset_name=\"filtered-cured-50k-global-fixed-base-loss\"\n",
    "# train_dataset_name=\"filtered-cured-50k-active-split-token_ranking_sample-fixed-base-loss\"\n",
    "# train_dataset_name=\"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss\"\n",
    "# train_dataset_name=\"filtered-cured-50k-active-split-global-curve-positive-fixed-base-loss-using-warmup-full-data\"\n",
    "# train_dataset_name=\"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup-llama8b\"\n",
    "\n",
    "train_dataset_name=\"filtered-cured-50k-active-split-data-prop-0.6-fixed-base-loss-using-warmup-llama3b\"\n",
    "\n",
    "\n",
    "ref_name = os.path.basename(reference_model_name_or_path)\n",
    "base_name =  os.path.basename(base_model_name_or_path)\n",
    "\n",
    "reference_losses = torch.load(data_path + f\"token_losses_filtered-cured-50k-rho-baseline_{ref_name}.pt\")\n",
    "\n",
    "base_losses = torch.load(data_path + f\"token_losses_filtered-cured-50k-rho-baseline_{base_name}.pt\")\n",
    "\n",
    "num_subset = 5\n",
    "subset_size = int(len(reference_losses) / num_subset)\n",
    "\n",
    "for idx in range(num_subset):\n",
    "    \n",
    "    train_dataset_name_tag = f\"{train_dataset_name}_{idx}\"\n",
    "    \n",
    "    subset_ref_losses = reference_losses[idx*subset_size: (idx+1)*subset_size]\n",
    "    subset_base_losses = base_losses[idx*subset_size: (idx+1)*subset_size]\n",
    "    \n",
    "    merged_base_model_name = f\"lora_merged_{train_dataset_name}_{idx-1}\" if idx > 0 else base_name\n",
    "    \n",
    "    torch.save(subset_base_losses, data_path + f\"token_losses_{train_dataset_name_tag}_{merged_base_model_name}.pt\")\n",
    "    torch.save(subset_ref_losses, data_path + f\"token_losses_{train_dataset_name_tag}_{ref_name}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly combine iter split pattern label  and other types' pattern label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_569443/273071572.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  orig_labels_all =  torch.load(label_path + f\"token_labels_{orig_train_data_tag}_{idx}.pt\")\n",
      "/tmp/ipykernel_569443/273071572.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  iter_labels_all =  torch.load(label_path + f\"token_labels_{iter_train_data_tag}_{idx}.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_subset=5\n",
    "\n",
    "label_path='results/label/'\n",
    "\n",
    "\n",
    "## combination 1\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-curve-positive-fixed-base-loss-using-warmup\"\n",
    "# new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_combine_active-split-global-curve-positive-fixed-base-loss-using-warmup-label\"\n",
    "\n",
    "## combination 2\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss\"\n",
    "# new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_combine_active-split-global-curve-positive-fixed-base-loss-label\" ## wrong text label, which should global-half-positive\n",
    "\n",
    "# ## combination 4\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss\"\n",
    "# new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-label\"\n",
    "\n",
    "\n",
    "# ## llama-8b\n",
    "iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_llama8b\"\n",
    "orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup-llama8b\"\n",
    "new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_llama8b\"\n",
    "\n",
    "\n",
    "### mistral-7b\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_mistral\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup-mistral\"\n",
    "# new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_mistral\"\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(1, num_subset):\n",
    "    orig_labels_all =  torch.load(label_path + f\"token_labels_{orig_train_data_tag}_{idx}.pt\")\n",
    "    iter_labels_all =  torch.load(label_path + f\"token_labels_{iter_train_data_tag}_{idx}.pt\")\n",
    "\n",
    "    selected_labels = [[-100 for _ in range(len(label))] for label in orig_labels_all]\n",
    "    \n",
    "    for i, (orig_labels_per_sample, iter_labels_per_sample) in enumerate(zip(orig_labels_all, iter_labels_all)):\n",
    "        for j, (cur_label, additional_label) in enumerate(zip(orig_labels_per_sample, iter_labels_per_sample)):\n",
    "            if cur_label != -100 or additional_label != -100:\n",
    "                selected_labels[i][j] = cur_label if cur_label != -100 else additional_label\n",
    "        \n",
    "    \n",
    "    torch.save(selected_labels, label_path + f\"token_labels_{new_train_data_tag}_{idx}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine three types of token together: warmup label + iter pattern + llama-8b-inst label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2911754/808857797.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  orig_labels_all =  torch.load(label_path + f\"token_labels_{orig_train_data_tag}_{idx}.pt\")\n",
      "/tmp/ipykernel_2911754/808857797.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  iter_labels_all =  torch.load(label_path + f\"token_labels_{iter_train_data_tag}_{idx}.pt\")\n",
      "/tmp/ipykernel_2911754/808857797.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  additional_labels_all = torch.load(label_path + f\"token_labels_{additional_train_data_tag}_{idx}.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_subset=5\n",
    "\n",
    "label_path='results/label/'\n",
    "\n",
    "\n",
    "## combination 1\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss\"\n",
    "# additional_train_data_tag =  \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup\"\n",
    "# new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.6_combine_other_two_types_label\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## combination 2\n",
    "iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3\"\n",
    "orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss\"\n",
    "additional_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup\"\n",
    "new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_other_two_types_label\"\n",
    "\n",
    "\n",
    "for idx in range(1, num_subset):\n",
    "    orig_labels_all =  torch.load(label_path + f\"token_labels_{orig_train_data_tag}_{idx}.pt\")\n",
    "    iter_labels_all =  torch.load(label_path + f\"token_labels_{iter_train_data_tag}_{idx}.pt\")\n",
    "    additional_labels_all = torch.load(label_path + f\"token_labels_{additional_train_data_tag}_{idx}.pt\")\n",
    "    \n",
    "    selected_labels = [[-100 for _ in range(len(label))] for label in orig_labels_all]\n",
    "    \n",
    "    for i, (orig_labels_per_sample, iter_labels_per_sample, additional_labels_per_sample) in enumerate(zip(orig_labels_all, iter_labels_all, additional_labels_all)):\n",
    "        for j, (orig_label, iter_label, additional_label) in enumerate(zip(orig_labels_per_sample, iter_labels_per_sample, additional_labels_per_sample)):\n",
    "            \n",
    "            selected_labels[i][j] = int(max(-100, orig_label, iter_label, additional_label))\n",
    "        \n",
    "    \n",
    "    torch.save(selected_labels, label_path + f\"token_labels_{new_train_data_tag}_{idx}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all labels into one label file for once finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_270681/857014187.py:158: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cur_labels=torch.load(f\"results/label/token_labels_{train_data_tag}_{idx}.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm \n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, add_bos=False):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    if add_bos:\n",
    "        example_text = tokenizer.bos_token + example_text\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            labels[:, message_start_idx:message_end_idx] = -100\n",
    "\n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_data(raw_dataset):\n",
    "    \n",
    "    if \"prompt\" in raw_dataset[\"train\"].column_names and \"completion\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_prompt_completion_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "    elif \"messages\" in raw_dataset[\"train\"].column_names:\n",
    "        encode_function = partial(\n",
    "            encode_with_messages_format,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length= 2048,\n",
    "            add_bos= False,\n",
    "        )\n",
    "        \n",
    "            \n",
    "    lm_datasets = raw_dataset.map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        # remove_columns=[name for name in raw_dataset[\"train\"].column_names if name not in [\"idx\", \"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "\n",
    "    return lm_datasets['train']['labels']\n",
    "    \n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "\n",
    "# ## llama-3b\n",
    "base_model=\"meta-llama/Llama-3.2-3B\"\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_llama3b\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup-llama3b\"\n",
    "# new_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_llama3b\"\n",
    "train_data_tag=\"filtered-cured-50k-iter-split-global_data_prop_0.6_llama3b-non-filtered-fixed-base-model\"\n",
    "# ## llama-8b\n",
    "# base_model=\"meta-llama/Llama-3.1-8B\"\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_llama8b\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup-llama8b\"\n",
    "# train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_llama8b\"\n",
    "\n",
    "### mistral-7b\n",
    "# base_model=\"mistralai/Mistral-7B-v0.3\"\n",
    "# iter_train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_mistral\"\n",
    "# orig_train_data_tag = \"filtered-cured-50k-active-split-global-half-positive-fixed-base-loss-using-warmup-mistral\"\n",
    "# train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_mistral\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "raw_dataset = load_dataset(\"json\", data_files=\"selected_data/filtered-cured-10k-warmup.json\")\n",
    "warmup_labels = encode_data(raw_dataset)\n",
    "\n",
    "all_labels = []\n",
    "# all_labels.extend(warmup_labels) ## add the 10k labels\n",
    "\n",
    "###need to add the first round labels (all)\n",
    "for idx in range(1, 5):\n",
    "    \n",
    "    cur_labels=torch.load(f\"results/label/token_labels_{train_data_tag}_{idx}.pt\")\n",
    "    all_labels.extend(cur_labels)\n",
    "\n",
    "torch.save(all_labels, f\"results/label/token_labels_{train_data_tag}_all.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076b3d4d16c04b57b965ecf1580aea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "101496362"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files=\"selected_data/filtered-cured-50k_dataset.json\")['train']\n",
    "\n",
    "subset = datasets.select(list(range(10000, 50000)))\n",
    "subset.to_json(\"filtered-cured-50k-iter-split-global_data_prop_0.6_llama3b-non-filtered-fixed-base-model_all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1822117/1748262393.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_labels = torch.load(f\"results/label/token_labels_{train_data_tag}_all.pt\")\n"
     ]
    }
   ],
   "source": [
    "train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_mistral\"\n",
    "import torch\n",
    "# train_data_tag = \"filtered-cured-50k-iter-split-global_data_prop_0.3_combine_active-split-global-half-positive-fixed-base-loss-using-warmup-label_llama8b\"\n",
    "\n",
    "all_labels = torch.load(f\"results/label/token_labels_{train_data_tag}_all.pt\")\n",
    "\n",
    "count_selected_tokens = []\n",
    "for sample_label in all_labels:\n",
    "    count_selected_token_per_sample = sum([1 for label in sample_label if label != -100])\n",
    "    count_selected_tokens.append(count_selected_token_per_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split token labels into several files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_809831/2496122999.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels_all = torch.load(label_path + f\"token_labels_{train_dataset_tag}.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_dataset_tag=\"filtered-cured-50k-rho-baseline-llama3b-global\"\n",
    "label_path =\"results/label/\"\n",
    "\n",
    "labels_all = torch.load(label_path + f\"token_labels_{train_dataset_tag}.pt\")\n",
    "\n",
    "subset_size= 10000\n",
    "\n",
    "for idx in [1,2,3,4]:\n",
    "    cur_idx_labels = labels_all[subset_size*idx: 10000 *(subset_size+1)]\n",
    "    torch.save(cur_idx_labels, label_path + f\"token_labels_{train_dataset_tag}_{idx}.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载本地 JSON 文件为数据集（请将 \"path/to/your.json\" 替换为你实际的文件路径）\n",
    "dataset = load_dataset(\"json\", data_files=\"selected_data/filtered-cured-50k_dataset.json\", split=\"train\")\n",
    "\n",
    "# 推送数据集到 Hugging Face Hub（替换 your_username/your_dataset_name 为你的数据集仓库名称）\n",
    "dataset.push_to_hub(\"jlpang888/DS2_50k\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
